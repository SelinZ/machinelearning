{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df345a31-d947-463d-832d-68a5175e4a7d",
   "metadata": {},
   "source": [
    "Building the dataset of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89ab51f-d954-42fa-bf20-80ea54b6757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows printing full text\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fd4ad-322a-4f6e-af15-7194165b3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PUT MAIN HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b81166-b7ea-4668-a3d2-c09b9a9b246d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIMPLEMENT regression models fuctions here\\n- exponential\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SETUP\n",
    "\"\"\"\n",
    "### Import models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "### Import self-made functions\n",
    "from CODE.data_preprocessing.split_val import split_val\n",
    "from CODE.data_preprocessing.find_outliers_tukey import find_outliers_tukey\n",
    "from CODE.features.length_title import length_title\n",
    "from CODE.features.field_variety import field_variety2\n",
    "#from CODE.features.field_variety import field_variety\n",
    "from CODE.features.team_size import team_size\n",
    "from CODE.features.topic_variety import topics_variety\n",
    "from CODE.features.venue_frequency import venue_frequency\n",
    "from CODE.features.age import age\n",
    "#from CODE.features.author_database import author_database\n",
    "#from CODE.features.author_name import author_name\n",
    "from CODE.features.abst_words import abst_words\n",
    "\n",
    "### Get the full train set:\n",
    "data = pd.read_json('DATA/train-1.json')   # Numerical columns: 'year', 'references', 'citations'\n",
    "test = pd.read_json('DATA/test.json')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DEAL with missing values in \"data\" and \"test\" here - SELIN\n",
    "\n",
    "doi --> \"\"\n",
    "title --> \"\"\n",
    "abstract --> \"\" \n",
    "authors --> [\"\"]\n",
    "venue --> \"\"\n",
    "year --> mean of venue based on \"data\" ELSE from \"data\"\n",
    "references --> 0  --think about this!\n",
    "topic --> [\"\"]\n",
    "is_open-access --> base on venue ELSE from \"data\"\n",
    "fields_of_study --> [\"\"]\n",
    "citations --> assume not blank\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### push the numerical columns to num_X\n",
    "end = len(data)\n",
    "num_X = data.loc[ 0:end+1 , ('doi', 'citations', 'year', 'references') ]  ##REMOVE DOI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "FEATURE DATAFRAME: num_X\n",
    "\n",
    "ALL: After writing a funtion to create a feature, please incorporate your new feature as a column on the dataframe below.\n",
    "This is the dataframe we will use to train the models.\n",
    "\"\"\"\n",
    "\n",
    "### use feature function to create a new variable\n",
    "\"\"\"\n",
    "DO NOT change the order in this section if at all possible\n",
    "\"\"\"\n",
    "title_len = length_title(data)      # returns: dictionary of lists: [doi](count)\n",
    "field_var = field_variety2(data)    # returns: dictionary of lists: [doi](count)\n",
    "team_sz = team_size(data)           # returns a numbered series\n",
    "topic_var = topics_variety(data)    # returns a numbered series\n",
    "venue_db, venues_reformatted = venue_frequency(data)  # returns a dictionary: [venue](count) and a pandas.Series of the 'venues' column reformatted \n",
    "num_X['venue'] = venues_reformatted # Dataframe needs a venue to deal with missing values\n",
    "paper_age = age(data)               # returns a numbered series\n",
    "open_access = pd.get_dummies(data[\"is_open_access\"], drop_first = True)  # returns pd.df (True = 1)\n",
    "\n",
    "keywords = [\"method\", \"review\", \"randomized\", \"random control\"]\n",
    "abst_keywords = abst_words(data, keywords)   #returns a numbered series: 1 if any of the words is present in the abstract, else 0\n",
    "\"\"\"\n",
    "END do not reorder\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### join the variables (type = series) to num_X \n",
    "num_X['team_size'] = team_sz\n",
    "num_X['topic_variety'] = topic_var\n",
    "num_X['age'] = paper_age\n",
    "num_X['open_access'] = open_access\n",
    "num_X['has_keyword'] = abst_keywords\n",
    "num_X['venue'] = venues_reformatted\n",
    "\n",
    "### join the variables (type = dictionary) to num_X\n",
    "num_X['title_length'] = num_X['doi'].map(title_len)\n",
    "num_X['field_variety'] = num_X['doi'].map(field_var)\n",
    "\n",
    "\n",
    "# Check venue and add venue_frequency to each paper\n",
    "venue_freq = pd.Series(dtype=pd.Int64Dtype())\n",
    "for index, i_paper in num_X.iterrows():\n",
    "    venue_freq[index,] = venue_db[i_paper['venue']] \n",
    "num_X['venue_freq'] = venue_freq\n",
    "\n",
    "\n",
    "### Drop columns containing just strings\n",
    "num_X = num_X.drop(['venue', 'doi'], axis = 1)\n",
    "\n",
    "\n",
    "## train/val split\n",
    "X_train, X_val, y_train, y_val = split_val(num_X, target_variable = 'citations')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "INSERT outlier detection on X_train here - ALBERT\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "IMPLEMENT regression models fuctions here\n",
    "- exponential\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0219f-cab9-48a4-86f8-bd4be754b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "------------------------------ LETS EXPLORE!!! ------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5af6a-3628-4117-9350-ec45300cf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(type(abst_keywords))\n",
    "print(abst_keywords)\n",
    "#num_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a68c3-1b25-47c6-803d-09c9a1fc0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR: exploring the scaffolding of the new dataframe for prediction as pulled from the full dataset\n",
    "\n",
    "# print(type(data))\n",
    "# print(list(data.columns))\n",
    "# print(\"X type:\", type(num_X), \"X shape:\", num_X.shape)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81b85a-733f-4174-934e-6404551e0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR: exploring the results of feature functions\n",
    "\n",
    "print(type(title_len))\n",
    "print(type(field_var))\n",
    "print(type(team_sz))\n",
    "print(type(topic_var))\n",
    "print(type(venue_freq))\n",
    "print(type(paper_age))\n",
    "#title_len\n",
    "#field_var\n",
    "#team_sz\n",
    "#topic_var\n",
    "#venue_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04915fc1-7323-404b-aa26-761a7b9ee7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR: exploring the new dataframe with numerical columns\n",
    "\n",
    "# from StackExchange:\n",
    "# Never grow a DataFrame! It is always cheaper to append to a python list and then \n",
    "# convert it to a DataFrame at the end, both in terms of memory and performance.\n",
    "# When appending to df, a new DataFrame is created each time in memory instead of \n",
    "# using the existing one, which is quite frankly a waste. It is always cheaper to \n",
    "# append to a python list and then convert it to a DataFrame at the end, both in \n",
    "# terms of memory and performance.\n",
    "\n",
    "# --> NOTE: it would be more efficient to combine these first and only expand the df once (per addition type)\n",
    "\n",
    "num_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a982b8-4ac9-41e1-bfe2-5265e574b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR: explore data train/val split  (should be 6470 train rows and 3188 validation rows)\n",
    "# names: X_train, X_val, y_train, y_val\n",
    "#X_train\n",
    "#X_val\n",
    "#y_train\n",
    "#y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c35830-e499-4409-a81b-e0fc78404744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "------------------------- LETS CODE!!! --------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b150ea0e-bae1-428c-8d1a-4f70df336c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: [901, 2580, 438, 7307, 9575, 579, 4041, 4241, 7663, 5688, 4326, 2476, 4830, 8603, 5288, 7567, 6435, 4844, 4149, 5328, 297, 2842, 4019, 8307, 2445, 6870, 5576, 90, 5114, 5231, 8862, 3682, 941, 2505, 3293, 8033, 6845, 3982, 720, 1358, 6810, 2292, 4515, 1987, 5868, 9277, 4500, 3824, 9617, 5961, 9251, 6840, 7983, 5894, 5939, 1023, 3492, 4743, 4236, 6768, 9475, 8176, 1257, 3546, 2606, 1819, 4305, 4987, 5480, 2007, 5322, 6026, 8752, 5223, 6165, 8794, 1753, 5201, 7535, 204, 7055, 6886, 2448, 7129, 2974, 9502, 9596, 2583, 5345, 2376, 4940, 6592, 9476, 5887, 4837, 7854, 4576, 148, 9604, 2774, 630, 7000, 8060, 5921, 4431, 2411, 6772, 2203, 3466, 4281, 7090, 3056, 2844, 4675, 3845, 8190, 8447, 3810, 3042, 4853, 9376, 4831, 2679, 9329, 4841, 9188, 1990, 6783, 6767, 9305, 6819, 1256, 6316, 4312, 8335, 9200, 7805, 8173, 537, 8096, 5423, 5191]\n",
      "X: [6985, 9498, 7439, 9030, 3643, 7249, 5723, 3589, 428, 4185, 9052, 7819, 5970, 3308, 4108, 298, 53, 6810, 2616, 8819, 8382, 9277, 3786, 9617, 3559, 7737, 4305, 5889, 6952, 4600, 459, 204, 3959, 8490, 8072, 4259, 2774, 8810, 2390, 3568, 6675, 4831, 6704, 126, 2136, 523, 5161, 9207, 9462, 8004]\n",
      "rows: [53, 90, 126, 148, 204, 297, 298, 428, 438, 459, 523, 537, 579, 630, 720, 901, 941, 1023, 1256, 1257, 1358, 1753, 1819, 1987, 1990, 2007, 2136, 2203, 2292, 2376, 2390, 2411, 2445, 2448, 2476, 2505, 2580, 2583, 2606, 2616, 2679, 2774, 2842, 2844, 2974, 3042, 3056, 3293, 3308, 3466, 3492, 3546, 3559, 3568, 3589, 3643, 3682, 3786, 3810, 3824, 3845, 3959, 3982, 4019, 4041, 4108, 4149, 4185, 4236, 4241, 4259, 4281, 4305, 4312, 4326, 4431, 4500, 4515, 4576, 4600, 4675, 4743, 4830, 4831, 4837, 4841, 4844, 4853, 4940, 4987, 5114, 5161, 5191, 5201, 5223, 5231, 5288, 5322, 5328, 5345, 5423, 5480, 5576, 5688, 5723, 5868, 5887, 5889, 5894, 5921, 5939, 5961, 5970, 6026, 6165, 6316, 6435, 6592, 6675, 6704, 6767, 6768, 6772, 6783, 6810, 6819, 6840, 6845, 6870, 6886, 6952, 6985, 7000, 7055, 7090, 7129, 7249, 7307, 7439, 7535, 7567, 7663, 7737, 7805, 7819, 7854, 7983, 8004, 8033, 8060, 8072, 8096, 8173, 8176, 8190, 8307, 8335, 8382, 8447, 8490, 8603, 8752, 8794, 8810, 8819, 8862, 9030, 9052, 9188, 9200, 9207, 9251, 9277, 9305, 9329, 9376, 9462, 9475, 9476, 9498, 9502, 9575, 9596, 9604, 9617]\n",
      "<class 'int'>\n",
      "(6470, 10)\n",
      "(6285, 10)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove outliers\n",
    "\"\"\"\n",
    "#names: X_train, X_val, y_train, y_val\n",
    "#print(list(X_train.columns))\n",
    "\n",
    "#print(\"citations:\", find_outliers_tukey(x = y_train['citations'], top = 93, bottom = 0))\n",
    "#print(\"citations:\", find_outliers_tukey(x = x_train['year'], top = 93, bottom = 0))\n",
    "\n",
    "out_y = (find_outliers_tukey(x = y_train['citations'], top = 93, bottom = 0))[0]\n",
    "out_X = (find_outliers_tukey(x = X_train['references'], top = 85, bottom = 0))[0]\n",
    "out_rows = out_y + out_X\n",
    "out_rows = sorted(list(set(out_rows)))\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train = X_train.drop(labels = out_rows)\n",
    "print(X_train.shape)\n",
    "\n",
    "#print(list(X_train.columns))\n",
    "# print(\"year:\", find_outliers_tukey(X_train['year']))\n",
    "# print(\"references:\", find_outliers_tukey(X_train['references']))\n",
    "# print(\"team_size:\", find_outliers_tukey(X_train['team_size']))\n",
    "# print(\"topic_variety:\", find_outliers_tukey(X_train['topic_variety']))\n",
    "# print(\"age:\", find_outliers_tukey(X_train['age']))\n",
    "# print(\"open_access:\", find_outliers_tukey(X_train['open_access']))\n",
    "# print(\"has_keyword:\", find_outliers_tukey(X_train['has_keyword']))\n",
    "# print(\"title_length:\", find_outliers_tukey(X_train['title_length']))\n",
    "# print(\"field_variety:\", find_outliers_tukey(X_train['field_variety']))\n",
    "# print(\"venue_freq:\", find_outliers_tukey(X_train['venue_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b3962-c6ba-4b6f-b23f-3ebfcb3cbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3 code to demonstrate \n",
    "# removing duplicated from list \n",
    "# using set()\n",
    "  \n",
    "# initializing list\n",
    "test_list = [1, 5, 3, 6, 3, 5, 6, 1]\n",
    "print (\"The original list is : \" +  str(test_list))\n",
    "  \n",
    "# using set()\n",
    "# to remove duplicated \n",
    "# from list \n",
    "test_list = list(set(test_list))\n",
    "  \n",
    "# printing list after removal \n",
    "# distorted ordering\n",
    "print (\"The list after removing duplicates : \" + str(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4828176-3b43-45e1-98ab-de2e109ee06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Look at some correlations\n",
    "\"\"\"\n",
    "# names: X_train, X_val, y_train, y_val\n",
    "\n",
    "corr_mat = num_X.corr(method='pearson')\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b96d8-8c71-4246-8d29-196132711d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic regression model Using any continuous variables\n",
    "#     Establish data\n",
    "#     Define model: regression model: sklearn.linear_model.LinearRegression\n",
    "#     Fit model\n",
    "#     Predict\n",
    "#     Evaluate\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "\n",
    "# 1. z-score\n",
    "reg = model.fit(X = X_train, y = y_train)  # 2. fit model\n",
    "print(\"Model weights:\", reg.coef_)\n",
    "print(\"Model intercept/bias:\", reg.intercept_)\n",
    "y_pred_val = model.predict(X_val)  # 3. predict\n",
    "a = r2_score(y_val, y_pred_val)  # 4. evaluate\n",
    "b = mean_absolute_error(y_val, y_pred_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2820d4b-4200-4214-bc23-27c872ae6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mini version of the main 'data' dataframe\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# %pwd\n",
    "# %cd C:\\Users\\r_noc\\Desktop\\Python\\GIT\\machinelearning\n",
    "    \n",
    "play = data.sample(100, replace = False, axis = 0)  \n",
    "\n",
    "\n",
    "print(play.shape)\n",
    "# print(play['abstract'])\n",
    "\n",
    "print(list(play.columns))\n",
    "# play['has_keyword'] = np.nan\n",
    "# print(play.shape)\n",
    "# play"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
